{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Classification using Naive Bayes and the IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDB Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful\n",
      "Extraction successful\n",
      "Extracted files: ['test.json', 'train.json']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the dataset (IMDB movie reviews)\n",
    "url = \"https://academy.hackthebox.com/storage/modules/292/skills_assessment_data.zip\"\n",
    "\n",
    "# Download the dataset\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Download successful\")\n",
    "else:\n",
    "    print(\"Failed to download the dataset\")\n",
    "\n",
    "# Extract the dataset to a folder called 'imdb_reviews'\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"imdb_reviews\")\n",
    "    print(\"Extraction successful\")\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = os.listdir(\"imdb_reviews\")\n",
    "print(\"Extracted files:\", extracted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- HEAD (Train) --------------------\n",
      "                                                text  label\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
      "1  Homelessness (or Houselessness as George Carli...      1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
      "3  This is easily the most underrated film inn th...      1\n",
      "4  This is not the typical Mel Brooks film. It wa...      1\n",
      "-------------------- DESCRIBE (Train) --------------------\n",
      "             label\n",
      "count  25000.00000\n",
      "mean       0.50000\n",
      "std        0.50001\n",
      "min        0.00000\n",
      "25%        0.00000\n",
      "50%        0.50000\n",
      "75%        1.00000\n",
      "max        1.00000\n",
      "-------------------- INFO (Train) --------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    25000 non-null  object\n",
      " 1   label   25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n",
      "None\n",
      "Missing values in training set:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Duplicate entries in training set: 96\n"
     ]
    }
   ],
   "source": [
    "# Adjust the file name and structure as needed\n",
    "train_df = pd.read_json(\"imdb_reviews/train.json\")  # Use `lines=True` if the JSON file is line-delimited\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"-------------------- HEAD (Train) --------------------\")\n",
    "print(train_df.head())\n",
    "print(\"-------------------- DESCRIBE (Train) --------------------\")\n",
    "print(train_df.describe())\n",
    "print(\"-------------------- INFO (Train) --------------------\")\n",
    "print(train_df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in training set:\\n\", train_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Duplicate entries in training set:\", train_df.duplicated().sum())\n",
    "\n",
    "# Remove duplicates if any\n",
    "train_df = train_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ANY PREPROCESSING (Train) ===\n",
      "                                                text  label\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
      "1  Homelessness (or Houselessness as George Carli...      1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
      "3  This is easily the most underrated film inn th...      1\n",
      "4  This is not the typical Mel Brooks film. It wa...      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"=== BEFORE ANY PREPROCESSING (Train) ===\") \n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER LOWERCASING (Train) ===\n",
      "0    bromwell high is a cartoon comedy. it ran at t...\n",
      "1    homelessness (or houselessness as george carli...\n",
      "2    brilliant over-acting by lesley ann warren. be...\n",
      "3    this is easily the most underrated film inn th...\n",
      "4    this is not the typical mel brooks film. it wa...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert all review text to lowercase\n",
    "train_df[\"text\"] = train_df[\"text\"].str.lower()\n",
    "print(\"\\n=== AFTER LOWERCASING (Train) ===\")\n",
    "print(train_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation and Numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING PUNCTUATION & NUMBERS (Train) ===\n",
      "0    bromwell high is a cartoon comedy it ran at th...\n",
      "1    homelessness or houselessness as george carlin...\n",
      "2    brilliant overacting by lesley ann warren best...\n",
      "3    this is easily the most underrated film inn th...\n",
      "4    this is not the typical mel brooks film it was...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Remove non-essential punctuation and numbers, keep symbols like $ and !\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (Train) ===\")\n",
    "print(train_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER TOKENIZATION (Train) ===\n",
      "0    [bromwell, high, is, a, cartoon, comedy, it, r...\n",
      "1    [homelessness, or, houselessness, as, george, ...\n",
      "2    [brilliant, overacting, by, lesley, ann, warre...\n",
      "3    [this, is, easily, the, most, underrated, film...\n",
      "4    [this, is, not, the, typical, mel, brooks, fil...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split each review into individual tokens\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(word_tokenize)\n",
    "print(\"\\n=== AFTER TOKENIZATION (Train) ===\")\n",
    "print(train_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING STOP WORDS (Train) ===\n",
      "0    [bromwell, high, cartoon, comedy, ran, time, p...\n",
      "1    [homelessness, houselessness, george, carlin, ...\n",
      "2    [brilliant, overacting, lesley, ann, warren, b...\n",
      "3    [easily, underrated, film, inn, brooks, cannon...\n",
      "4    [typical, mel, brooks, film, much, less, slaps...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a set of English stop words and remove them\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"\\n=== AFTER REMOVING STOP WORDS (Train) ===\")\n",
    "print(train_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER STEMMING (Train) ===\n",
      "0    [bromwel, high, cartoon, comedi, ran, time, pr...\n",
      "1    [homeless, houseless, georg, carlin, state, is...\n",
      "2    [brilliant, overact, lesley, ann, warren, best...\n",
      "3    [easili, underr, film, inn, brook, cannon, sur...\n",
      "4    [typic, mel, brook, film, much, less, slapstic...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Stem each token\n",
    "stemmer = PorterStemmer()\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(\"\\n=== AFTER STEMMING (Train) ===\")\n",
    "print(train_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Joining Tokens Back into a Single String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER JOINING TOKENS BACK INTO STRINGS (Train) ===\n",
      "0    bromwel high cartoon comedi ran time program s...\n",
      "1    homeless houseless georg carlin state issu yea...\n",
      "2    brilliant overact lesley ann warren best drama...\n",
      "3    easili underr film inn brook cannon sure flaw ...\n",
      "4    typic mel brook film much less slapstick movi ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rejoin tokens into a single string for feature extraction\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: \" \".join(x))\n",
    "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS (Train) ===\")\n",
    "print(train_df[\"text\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CountVectorizer for the Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer (bigrams, min_df=1, max_df=0.9 to focus on relevant terms)\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the review column\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "# Labels (target variable)\n",
    "# According to the prompt: positive=0, negative=1\n",
    "# If your 'label' column is already 0 or 1, you can just use it directly.\n",
    "# If not, map them accordingly.\n",
    "y_train = train_df[\"label\"].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best model parameters: {'vectorizer__ngram_range': (1, 3), 'vectorizer__min_df': 1, 'vectorizer__max_df': 0.9, 'classifier__solver': 'liblinear', 'classifier__C': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Build a pipeline that includes vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 3))),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"classifier__C\": [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    \"classifier__solver\": [\"lbfgs\", \"liblinear\"],  # Different solvers\n",
    "    \"vectorizer__ngram_range\": [(1, 2), (1, 3)],  # Bi-grams or Tri-grams\n",
    "    \"vectorizer__max_df\": [0.75, 0.9, 1.0],  # Remove too frequent terms\n",
    "    \"vectorizer__min_df\": [1, 2, 3]  # Remove rare terms\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "grid_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring=\"f1\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all processors\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "grid_search.fit(train_df[\"text\"], y_train)\n",
    "\n",
    "# Extract the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading & Preprocessing the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- HEAD (Test) --------------------\n",
      "                                                text  label\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
      "1  Homelessness (or Houselessness as George Carli...      1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
      "3  This is easily the most underrated film inn th...      1\n",
      "4  This is not the typical Mel Brooks film. It wa...      1\n"
     ]
    }
   ],
   "source": [
    "# Similar approach for test data\n",
    "test_df = pd.read_json(\"imdb_reviews/train.json\")\n",
    "\n",
    "print(\"\\n-------------------- HEAD (Test) --------------------\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Basic cleanup and checks\n",
    "test_df = test_df.drop_duplicates()\n",
    "test_df[\"text\"] = test_df[\"text\"].str.lower()\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(word_tokenize)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "X_test = best_model.named_steps[\"vectorizer\"].transform(test_df[\"text\"])\n",
    "y_test = test_df[\"label\"].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on test set: 1.0000\n",
      "F1-score on test set: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Ensure X_test contains the raw text (not preprocessed)\n",
    "# The pipeline will handle preprocessing and vectorization internally\n",
    "test_predictions = best_model.predict(test_df[\"text\"])  # Pass raw test reviews to the pipeline\n",
    "test_prediction_probabilities = best_model.predict_proba(test_df[\"text\"])  # Same here for probabilities\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "acc = accuracy_score(y_test, test_predictions)\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nAccuracy on test set: {acc:.4f}\")\n",
    "print(f\"F1-score on test set: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using joblib for Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to imdb_sentiment_nb_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = 'imdb_sentiment_nb_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
